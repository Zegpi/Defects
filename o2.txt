Start of PruebaV2 
h= 0.196078
nx<= 102 ny= 102
dt<= 0.098039     dt= 0.049020
Nt= 2
Mat Object: 1 MPI processes
  type: seqaij
row 0: (4, 1.) 
row 1: (5, 1.) 
row 2: (6, 1.) 
row 3: (7, 1.) 
row 4: (8, 1.) 
row 5: (9, 1.) 
Mat Object: 1 MPI processes
  type: seqaij
row 0: (0, 1.)  (1, 2.)  (2, 3.)  (3, 4.)  (4, 5.)  (5, 6.)  (6, 7.)  (7, 8.)  (8, 9.)  (9, 10.) 
row 1: (0, 11.)  (1, 12.)  (2, 13.)  (3, 14.)  (4, 15.)  (5, 16.)  (6, 17.)  (7, 18.)  (8, 19.)  (9, 20.) 
row 2: (0, 21.)  (1, 22.)  (2, 23.)  (3, 24.)  (4, 25.)  (5, 26.)  (6, 27.)  (7, 28.)  (8, 29.)  (9, 30.) 
row 3: (0, 31.)  (1, 32.)  (2, 33.)  (3, 34.)  (4, 35.)  (5, 36.)  (6, 37.)  (7, 38.)  (8, 39.)  (9, 40.) 
row 4: (0, 41.)  (1, 42.)  (2, 43.)  (3, 44.)  (4, 45.)  (5, 46.)  (6, 47.)  (7, 48.)  (8, 49.)  (9, 50.) 
row 5: (0, 51.)  (1, 52.)  (2, 53.)  (3, 54.)  (4, 55.)  (5, 56.)  (6, 57.)  (7, 58.)  (8, 59.)  (9, 60.) 
row 6: (0, 61.)  (1, 62.)  (2, 63.)  (3, 64.)  (4, 65.)  (5, 66.)  (6, 67.)  (7, 68.)  (8, 69.)  (9, 70.) 
row 7: (0, 71.)  (1, 72.)  (2, 73.)  (3, 74.)  (4, 75.)  (5, 76.)  (6, 77.)  (7, 78.)  (8, 79.)  (9, 80.) 
row 8: (0, 81.)  (1, 82.)  (2, 83.)  (3, 84.)  (4, 85.)  (5, 86.)  (6, 87.)  (7, 88.)  (8, 89.)  (9, 90.) 
row 9: (0, 91.)  (1, 92.)  (2, 93.)  (3, 94.)  (4, 95.)  (5, 96.)  (6, 97.)  (7, 98.)  (8, 99.)  (9, 100.) 
Mat Object: 1 MPI processes
  type: seqaij
row 0: (0, 41.)  (1, 42.)  (2, 43.)  (3, 44.)  (4, 45.)  (5, 46.)  (6, 47.)  (7, 48.)  (8, 49.)  (9, 50.) 
row 1: (0, 51.)  (1, 52.)  (2, 53.)  (3, 54.)  (4, 55.)  (5, 56.)  (6, 57.)  (7, 58.)  (8, 59.)  (9, 60.) 
row 2: (0, 61.)  (1, 62.)  (2, 63.)  (3, 64.)  (4, 65.)  (5, 66.)  (6, 67.)  (7, 68.)  (8, 69.)  (9, 70.) 
row 3: (0, 71.)  (1, 72.)  (2, 73.)  (3, 74.)  (4, 75.)  (5, 76.)  (6, 77.)  (7, 78.)  (8, 79.)  (9, 80.) 
row 4: (0, 81.)  (1, 82.)  (2, 83.)  (3, 84.)  (4, 85.)  (5, 86.)  (6, 87.)  (7, 88.)  (8, 89.)  (9, 90.) 
row 5: (0, 91.)  (1, 92.)  (2, 93.)  (3, 94.)  (4, 95.)  (5, 96.)  (6, 97.)  (7, 98.)  (8, 99.)  (9, 100.) 
Mat Object: 1 MPI processes
  type: seqaij
row 0: (0, 1.)   (1, 5.)   (2, 6.)   (3, 7.)   (4, 8.)   (5, 9.)   (6, 10.) 
row 1: (0, 11.)  (1, 15.)  (2, 16.)  (3, 17.)  (4, 18.)  (5, 19.)  (6, 20.) 
row 2: (0, 21.)  (1, 25.)  (2, 26.)  (3, 27.)  (4, 28.)  (5, 29.)  (6, 30.) 
row 3: (0, 31.)  (1, 35.)  (2, 36.)  (3, 37.)  (4, 38.)  (5, 39.)  (6, 40.) 
row 4: (0, 41.)  (1, 45.)  (2, 46.)  (3, 47.)  (4, 48.)  (5, 49.)  (6, 50.) 
row 5: (0, 51.)  (1, 55.)  (2, 56.)  (3, 57.)  (4, 58.)  (5, 59.)  (6, 60.) 
row 6: (0, 61.)  (1, 65.)  (2, 66.)  (3, 67.)  (4, 68.)  (5, 69.)  (6, 70.) 
row 7: (0, 71.)  (1, 75.)  (2, 76.)  (3, 77.)  (4, 78.)  (5, 79.)  (6, 80.) 
row 8: (0, 81.)  (1, 85.)  (2, 86.)  (3, 87.)  (4, 88.)  (5, 89.)  (6, 90.) 
row 9: (0, 91.)  (1, 95.)  (2, 96.)  (3, 97.)  (4, 98.)  (5, 99.)  (6, 100.) 
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------



      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


./PruebaV2 on a arch-linux2-c-debug named esteban-VBox with 1 processor, by eazegpi Mon Sep 23 20:38:20 2019
Using Petsc Release Version 3.9.3, unknown 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.137e-01      1.00000   4.137e-01
Objects:              7.000e+00      1.00000   7.000e+00
Flop:                 3.900e+02      1.00000   3.900e+02  3.900e+02
Flop/sec:            9.427e+02      1.00000   9.427e+02  9.427e+02
Memory:               9.139e+04      1.00000              9.139e+04
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00      0.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.1368e-01 100.0%  3.9000e+02 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flop                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatAssemblyBegin       7 1.0 2.3842e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         7 1.0 1.5068e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                4 1.0 3.2697e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatMatMult             2 1.0 6.4063e-04 1.0 3.90e+02 1.0 0.0e+00 0.0e+00 0.0e+00  0100  0  0  0   0100  0  0  0     1
MatMatMultSym          2 1.0 4.6039e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMatMultNum          2 1.0 1.1444e-04 1.0 3.90e+02 1.0 0.0e+00 0.0e+00 0.0e+00  0100  0  0  0   0100  0  0  0     3
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              0            0     0.
              Viewer     2              1          840     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
#PETSc Option Table entries:
-iga_view
-ksp_converged_reason
-ksp_monitor
-ksp_view
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-cc=gcc --with-fc=gfortran --download-mpich --download-fblaslapack --download-scalapack --download-mumps --download-parmetis --download-metis --download-ptscotch --download-hypre --download-ml --download-cmake
-----------------------------------------
Libraries compiled on 2018-07-23 16:56:16 on esteban-VBox 
Machine characteristics: Linux-4.15.0-29-generic-x86_64-with-Ubuntu-18.04-bionic
Using PETSc directory: /home/eazegpi/petsc
Using PETSc arch: arch-linux2-c-debug
-----------------------------------------

Using C compiler: /home/eazegpi/petsc/arch-linux2-c-debug/bin/mpicc    -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g3  
Using Fortran compiler: /home/eazegpi/petsc/arch-linux2-c-debug/bin/mpif90   -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g    
-----------------------------------------

Using include paths: -I/home/eazegpi/petsc/include -I/home/eazegpi/petsc/arch-linux2-c-debug/include
-----------------------------------------

Using C linker: /home/eazegpi/petsc/arch-linux2-c-debug/bin/mpicc
Using Fortran linker: /home/eazegpi/petsc/arch-linux2-c-debug/bin/mpif90
Using libraries: -Wl,-rpath,/home/eazegpi/petsc/arch-linux2-c-debug/lib -L/home/eazegpi/petsc/arch-linux2-c-debug/lib -lpetsc -Wl,-rpath,/home/eazegpi/petsc/arch-linux2-c-debug/lib -L/home/eazegpi/petsc/arch-linux2-c-debug/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lHYPRE -lml -lflapack -lfblas -lparmetis -lmetis -lptesmumps -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lm -lpthread -lstdc++ -ldl -lmpifort -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lrt -lm -lpthread -lz -lstdc++ -ldl
-----------------------------------------



      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


WARNING! There are options you set that were not used!
WARNING! could be spelling mistake, etc!
Option left: name:-iga_view (no value)
Option left: name:-ksp_converged_reason (no value)
Option left: name:-ksp_monitor (no value)
Option left: name:-ksp_view (no value)
