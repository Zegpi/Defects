Start of PruebaV5 
Current time is 13:48:55 

System for initial state of Pi starting 

IGA: dim=2 dof=4 order=1 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[1,0] rule=LEGENDRE[4] periodic=0 nnp=302 nel=301
Axis 1: basis=BSPLINE[1,0] rule=LEGENDRE[4] periodic=0 nnp=302 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=91204 min=7500 max=7676 max/min=1.02347
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347

System for curl part of Helmholtz of S starting 

Current time is 13:48:55 
IGA: dim=2 dof=8 order=1 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[1,0] rule=LEGENDRE[4] periodic=0 nnp=302 nel=301
Axis 1: basis=BSPLINE[1,0] rule=LEGENDRE[4] periodic=0 nnp=302 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=91204 min=7500 max=7676 max/min=1.02347
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347

System for L2 projection for Alfa+Sp:X starting 

Current time is 13:55:33 
IGA: dim=2 dof=2 order=1 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[1,0] rule=LEGENDRE[4] periodic=0 nnp=302 nel=301
Axis 1: basis=BSPLINE[1,0] rule=LEGENDRE[4] periodic=0 nnp=302 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=91204 min=7500 max=7676 max/min=1.02347
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347
  0 KSP Residual norm 0.000000000000e+00 
Linear solve converged due to CONVERGED_ATOL iterations 0
KSP Object: 12 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-25, absolute=1e-40, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 12 MPI processes
  type: bjacobi
    number of blocks = 12
    Local solve is same for all blocks, in the following KSP and PC objects:
  KSP Object: (sub_) 1 MPI processes
    type: preonly
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using NONE norm type for convergence test
  PC Object: (sub_) 1 MPI processes
    type: ilu
      out-of-place factorization
      0 levels of fill
      tolerance for zero pivot 2.22045e-14
      matrix ordering: natural
      factor fill ratio given 1., needed 1.
        Factored matrix follows:
          Mat Object: 1 MPI processes
            type: seqbaij
            rows=15352, cols=15352, bs=2
            package used to perform factorization: petsc
            total: nonzeros=272104, allocated nonzeros=272104
            total number of mallocs used during MatSetValues calls=0
                block size is 2
    linear system matrix = precond matrix:
    Mat Object: 1 MPI processes
      type: seqbaij
      rows=15352, cols=15352, bs=2
      total: nonzeros=272104, allocated nonzeros=272104
      total number of mallocs used during MatSetValues calls=0
          block size is 2
  linear system matrix = precond matrix:
  Mat Object: 12 MPI processes
    type: mpibaij
    rows=182408, cols=182408, bs=2
    total: nonzeros=3268864, allocated nonzeros=3268864
    total number of mallocs used during MatSetValues calls=0

System for curl part of Helmholtz of Up starting 

Current time is 13:55:35 
IGA: dim=2 dof=4 order=2 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[2,1] rule=LEGENDRE[4] periodic=0 nnp=303 nel=301
Axis 1: basis=BSPLINE[2,1] rule=LEGENDRE[4] periodic=0 nnp=303 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=91809 min=7500 max=7854 max/min=1.0472
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347

System for Z0 starting 

Current time is 13:59:08 
IGA: dim=2 dof=2 order=3 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Axis 1: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=92416 min=7500 max=8034 max/min=1.0712
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347

System for Stress starting 

Current time is 14:07:22 
IGA: dim=2 dof=4 order=2 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Axis 1: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=92416 min=7500 max=8034 max/min=1.0712
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347
  0 KSP Residual norm 3.253981153543e+01 
  1 KSP Residual norm 9.813724932883e+00 
  2 KSP Residual norm 2.323448862301e+00 
  3 KSP Residual norm 1.204359838599e+00 
  4 KSP Residual norm 5.165200397165e-02 
  5 KSP Residual norm 6.616391910022e-03 
  6 KSP Residual norm 3.443657968790e-04 
  7 KSP Residual norm 7.911915319036e-05 
  8 KSP Residual norm 1.358717560210e-05 
  9 KSP Residual norm 1.103832359773e-05 
 10 KSP Residual norm 3.186838087740e-06 
 11 KSP Residual norm 8.903655541295e-07 
 12 KSP Residual norm 1.994516758259e-09 
 13 KSP Residual norm 2.305327018359e-10 
 14 KSP Residual norm 4.423886309519e-11 
 15 KSP Residual norm 1.880831374278e-11 
 16 KSP Residual norm 4.151944873901e-13 
 17 KSP Residual norm 2.938578104072e-13 
 18 KSP Residual norm 2.400089160408e-13 
 19 KSP Residual norm 2.078861381355e-13 
 20 KSP Residual norm 1.859563846113e-13 
 21 KSP Residual norm 1.697647494555e-13 
 22 KSP Residual norm 1.571786096924e-13 
 23 KSP Residual norm 1.470320326669e-13 
 24 KSP Residual norm 1.386267277936e-13 
 25 KSP Residual norm 1.315155921973e-13 
 26 KSP Residual norm 1.253973275555e-13 
 27 KSP Residual norm 1.200605005475e-13 
 28 KSP Residual norm 1.153517797923e-13 
 29 KSP Residual norm 1.111568978559e-13 
 30 KSP Residual norm 6.562684208630e-13 
 31 KSP Residual norm 2.871283567577e-13 
 32 KSP Residual norm 1.256039111905e-13 
 33 KSP Residual norm 4.939610422671e-14 
 34 KSP Residual norm 2.997440616860e-14 
 35 KSP Residual norm 1.504609414360e-14 
 36 KSP Residual norm 9.129358617814e-15 
 37 KSP Residual norm 1.509731540065e-15 
 38 KSP Residual norm 9.960652087712e-17 
 39 KSP Residual norm 1.418248585624e-17 
 40 KSP Residual norm 3.747201551456e-18 
 41 KSP Residual norm 8.328833903175e-20 
Linear solve converged due to CONVERGED_RTOL iterations 41
KSP Object: 12 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-20, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 12 MPI processes
  type: bjacobi
    number of blocks = 12
    Local solve is same for all blocks, in the following KSP and PC objects:
  KSP Object: (sub_) 1 MPI processes
    type: preonly
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using NONE norm type for convergence test
  PC Object: (sub_) 1 MPI processes
    type: ilu
      out-of-place factorization
      0 levels of fill
      tolerance for zero pivot 2.22045e-14
      matrix ordering: natural
      factor fill ratio given 1., needed 1.
        Factored matrix follows:
          Mat Object: 1 MPI processes
            type: seqbaij
            rows=30704, cols=30704, bs=4
            package used to perform factorization: petsc
            total: nonzeros=5782400, allocated nonzeros=5782400
            total number of mallocs used during MatSetValues calls=0
                block size is 4
    linear system matrix = precond matrix:
    Mat Object: 1 MPI processes
      type: seqbaij
      rows=30704, cols=30704, bs=4
      total: nonzeros=5782400, allocated nonzeros=5782400
      total number of mallocs used during MatSetValues calls=0
          block size is 4
  linear system matrix = precond matrix:
  Mat Object: 12 MPI processes
    type: mpibaij
    rows=369664, cols=369664, bs=4
    total: nonzeros=71639296, allocated nonzeros=71639296
    total number of mallocs used during MatSetValues calls=0

System for Classic Stress starting 

Current time is 14:07:40 
IGA: dim=2 dof=4 order=2 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Axis 1: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=92416 min=7500 max=8034 max/min=1.0712
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347
  0 KSP Residual norm 3.706687705756e+01 
  1 KSP Residual norm 1.088575650247e+01 
  2 KSP Residual norm 2.050840688161e+00 
  3 KSP Residual norm 6.977696669969e-01 
  4 KSP Residual norm 5.301929374932e-02 
  5 KSP Residual norm 6.647048335978e-03 
  6 KSP Residual norm 3.500928868342e-04 
  7 KSP Residual norm 8.294690628335e-05 
  8 KSP Residual norm 1.364261474611e-05 
  9 KSP Residual norm 1.108621421521e-05 
 10 KSP Residual norm 3.196278558388e-06 
 11 KSP Residual norm 8.928897480219e-07 
 12 KSP Residual norm 2.003122178272e-09 
 13 KSP Residual norm 2.308526889719e-10 
 14 KSP Residual norm 4.394767483126e-11 
 15 KSP Residual norm 1.866965535893e-11 
 16 KSP Residual norm 1.771285771636e-13 
 17 KSP Residual norm 1.251920244295e-13 
 18 KSP Residual norm 1.022070231570e-13 
 19 KSP Residual norm 8.850814967136e-14 
 20 KSP Residual norm 7.916108984251e-14 
 21 KSP Residual norm 7.226202170172e-14 
 22 KSP Residual norm 6.690041870329e-14 
 23 KSP Residual norm 6.257876014975e-14 
 24 KSP Residual norm 5.899919664733e-14 
 25 KSP Residual norm 5.597107871164e-14 
 26 KSP Residual norm 5.336596006777e-14 
 27 KSP Residual norm 5.109372194752e-14 
 28 KSP Residual norm 4.908902049970e-14 
 29 KSP Residual norm 4.730316579114e-14 
 30 KSP Residual norm 5.283882379363e-13 
 31 KSP Residual norm 1.294658954073e-13 
 32 KSP Residual norm 5.123619236063e-14 
 33 KSP Residual norm 2.780216737720e-14 
 34 KSP Residual norm 9.631429268745e-15 
 35 KSP Residual norm 5.325671369792e-15 
 36 KSP Residual norm 4.111243359552e-15 
 37 KSP Residual norm 6.180772230649e-16 
Linear solve converged due to CONVERGED_RTOL iterations 37
KSP Object: 12 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-16, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 12 MPI processes
  type: bjacobi
    number of blocks = 12
    Local solve is same for all blocks, in the following KSP and PC objects:
  KSP Object: (sub_) 1 MPI processes
    type: preonly
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using NONE norm type for convergence test
  PC Object: (sub_) 1 MPI processes
    type: ilu
      out-of-place factorization
      0 levels of fill
      tolerance for zero pivot 2.22045e-14
      matrix ordering: natural
      factor fill ratio given 1., needed 1.
        Factored matrix follows:
          Mat Object: 1 MPI processes
            type: seqbaij
            rows=30704, cols=30704, bs=4
            package used to perform factorization: petsc
            total: nonzeros=5782400, allocated nonzeros=5782400
            total number of mallocs used during MatSetValues calls=0
                block size is 4
    linear system matrix = precond matrix:
    Mat Object: 1 MPI processes
      type: seqbaij
      rows=30704, cols=30704, bs=4
      total: nonzeros=5782400, allocated nonzeros=5782400
      total number of mallocs used during MatSetValues calls=0
          block size is 4
  linear system matrix = precond matrix:
  Mat Object: 12 MPI processes
    type: mpibaij
    rows=369664, cols=369664, bs=4
    total: nonzeros=71639296, allocated nonzeros=71639296
    total number of mallocs used during MatSetValues calls=0

System for CoupleStress starting 

Current time is 14:07:58 
IGA: dim=2 dof=2 order=2 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Axis 1: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=92416 min=7500 max=8034 max/min=1.0712
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347
  0 KSP Residual norm 1.187968684356e+00 
  1 KSP Residual norm 3.120142468198e-01 
  2 KSP Residual norm 5.811081691325e-02 
  3 KSP Residual norm 2.507776259414e-02 
  4 KSP Residual norm 2.035518288074e-03 
  5 KSP Residual norm 2.254031605844e-05 
  6 KSP Residual norm 4.894121593137e-06 
  7 KSP Residual norm 4.231152133876e-07 
  8 KSP Residual norm 5.127571863604e-08 
  9 KSP Residual norm 4.177373422204e-08 
 10 KSP Residual norm 1.063440044749e-08 
 11 KSP Residual norm 2.320486607537e-09 
 12 KSP Residual norm 7.681215603671e-12 
 13 KSP Residual norm 6.409448819410e-13 
 14 KSP Residual norm 1.484036912479e-13 
 15 KSP Residual norm 5.302990453206e-14 
 16 KSP Residual norm 1.690333222080e-15 
 17 KSP Residual norm 1.194986678393e-15 
 18 KSP Residual norm 9.756976911838e-16 
 19 KSP Residual norm 8.449654765527e-16 
 20 KSP Residual norm 7.557541597411e-16 
 21 KSP Residual norm 6.899022538612e-16 
 22 KSP Residual norm 6.387229067649e-16 
 23 KSP Residual norm 5.974688390701e-16 
 24 KSP Residual norm 5.632977539270e-16 
 25 KSP Residual norm 5.343902074139e-16 
 26 KSP Residual norm 5.095203191490e-16 
 27 KSP Residual norm 4.878279657765e-16 
 28 KSP Residual norm 4.686894616523e-16 
 29 KSP Residual norm 4.516400634287e-16 
 30 KSP Residual norm 7.215066721996e-15 
 31 KSP Residual norm 1.660783977260e-15 
 32 KSP Residual norm 7.688488673181e-16 
 33 KSP Residual norm 5.016244319916e-16 
 34 KSP Residual norm 1.350019083754e-17 
 35 KSP Residual norm 6.314897669718e-18 
 36 KSP Residual norm 4.800693564508e-18 
 37 KSP Residual norm 2.795814451214e-18 
 38 KSP Residual norm 1.370194459411e-19 
 39 KSP Residual norm 4.680315047274e-20 
 40 KSP Residual norm 2.108096465439e-20 
 41 KSP Residual norm 2.136895706330e-22 
 42 KSP Residual norm 7.819094681732e-23 
 43 KSP Residual norm 6.442546418044e-25 
Linear solve converged due to CONVERGED_RTOL iterations 43
KSP Object: 12 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-24, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 12 MPI processes
  type: bjacobi
    number of blocks = 12
    Local solve is same for all blocks, in the following KSP and PC objects:
  KSP Object: (sub_) 1 MPI processes
    type: preonly
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using NONE norm type for convergence test
  PC Object: (sub_) 1 MPI processes
    type: ilu
      out-of-place factorization
      0 levels of fill
      tolerance for zero pivot 2.22045e-14
      matrix ordering: natural
      factor fill ratio given 1., needed 1.
        Factored matrix follows:
          Mat Object: 1 MPI processes
            type: seqbaij
            rows=15352, cols=15352, bs=2
            package used to perform factorization: petsc
            total: nonzeros=1445600, allocated nonzeros=1445600
            total number of mallocs used during MatSetValues calls=0
                block size is 2
    linear system matrix = precond matrix:
    Mat Object: 1 MPI processes
      type: seqbaij
      rows=15352, cols=15352, bs=2
      total: nonzeros=1445600, allocated nonzeros=1445600
      total number of mallocs used during MatSetValues calls=0
          block size is 2
  linear system matrix = precond matrix:
  Mat Object: 12 MPI processes
    type: mpibaij
    rows=184832, cols=184832, bs=2
    total: nonzeros=17909824, allocated nonzeros=17909824
    total number of mallocs used during MatSetValues calls=0

System for Energy Density starting 

Current time is 14:08:10 
IGA: dim=2 dof=1 order=2 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Axis 1: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=92416 min=7500 max=8034 max/min=1.0712
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347

System for V-alpha starting 

Current time is 14:08:36 
IGA: dim=2 dof=2 order=1 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[1,0] rule=LEGENDRE[4] periodic=0 nnp=302 nel=301
Axis 1: basis=BSPLINE[1,0] rule=LEGENDRE[4] periodic=0 nnp=302 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=91204 min=7500 max=7676 max/min=1.02347
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347
  0 KSP Residual norm 1.191374046692e+03 
  1 KSP Residual norm 2.667485634131e+02 
  2 KSP Residual norm 2.887293013321e+01 
  3 KSP Residual norm 1.217078628074e-12 
  4 KSP Residual norm 8.605634686990e-13 
  5 KSP Residual norm 7.026412147841e-13 
  6 KSP Residual norm 6.085008541852e-13 
  7 KSP Residual norm 5.442576562548e-13 
  8 KSP Residual norm 4.968356998247e-13 
  9 KSP Residual norm 4.599796284985e-13 
 10 KSP Residual norm 4.302709474716e-13 
 11 KSP Residual norm 4.056629025068e-13 
 12 KSP Residual norm 3.848452885015e-13 
 13 KSP Residual norm 3.669353264389e-13 
 14 KSP Residual norm 3.513136218454e-13 
 15 KSP Residual norm 3.375310421763e-13 
 16 KSP Residual norm 3.252529120287e-13 
 17 KSP Residual norm 3.142240496482e-13 
 18 KSP Residual norm 3.042460294673e-13 
 19 KSP Residual norm 2.951619194992e-13 
 20 KSP Residual norm 2.868457529067e-13 
 21 KSP Residual norm 2.791950907570e-13 
 22 KSP Residual norm 2.721256576115e-13 
 23 KSP Residual norm 2.655674000595e-13 
 24 KSP Residual norm 2.594615426097e-13 
 25 KSP Residual norm 2.537583557759e-13 
 26 KSP Residual norm 2.484154413136e-13 
 27 KSP Residual norm 2.433963987071e-13 
 28 KSP Residual norm 2.386697766185e-13 
 29 KSP Residual norm 2.342082400183e-13 
 30 KSP Residual norm 1.188169864691e-12 
 31 KSP Residual norm 2.980407776782e-13 
 32 KSP Residual norm 2.847867903202e-14 
 33 KSP Residual norm 5.019864063388e-28 
Linear solve converged due to CONVERGED_RTOL iterations 33
KSP Object: 12 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-24, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 12 MPI processes
  type: bjacobi
    number of blocks = 12
    Local solve is same for all blocks, in the following KSP and PC objects:
  KSP Object: (sub_) 1 MPI processes
    type: preonly
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using NONE norm type for convergence test
  PC Object: (sub_) 1 MPI processes
    type: ilu
      out-of-place factorization
      0 levels of fill
      tolerance for zero pivot 2.22045e-14
      matrix ordering: natural
      factor fill ratio given 1., needed 1.
        Factored matrix follows:
          Mat Object: 1 MPI processes
            type: seqbaij
            rows=15352, cols=15352, bs=2
            package used to perform factorization: petsc
            total: nonzeros=272104, allocated nonzeros=272104
            total number of mallocs used during MatSetValues calls=0
                block size is 2
    linear system matrix = precond matrix:
    Mat Object: 1 MPI processes
      type: seqbaij
      rows=15352, cols=15352, bs=2
      total: nonzeros=272104, allocated nonzeros=272104
      total number of mallocs used during MatSetValues calls=0
          block size is 2
  linear system matrix = precond matrix:
  Mat Object: 12 MPI processes
    type: mpibaij
    rows=182408, cols=182408, bs=2
    total: nonzeros=3268864, allocated nonzeros=3268864
    total number of mallocs used during MatSetValues calls=0

System for Int(V-alpha) starting 

Current time is 14:09:01 
  0 KSP Residual norm 5.041272746718e+00 
  1 KSP Residual norm 5.966655740852e-01 
  2 KSP Residual norm 4.417224607047e-15 
  3 KSP Residual norm 3.108230278963e-15 
  4 KSP Residual norm 2.535490536245e-15 
  5 KSP Residual norm 2.194479486970e-15 
  6 KSP Residual norm 1.962128935182e-15 
  7 KSP Residual norm 1.790757258420e-15 
  8 KSP Residual norm 1.657646373246e-15 
  9 KSP Residual norm 1.550394933537e-15 
 10 KSP Residual norm 1.461586156766e-15 
 11 KSP Residual norm 1.386475995010e-15 
 12 KSP Residual norm 1.321870021418e-15 
 13 KSP Residual norm 1.265528004186e-15 
 14 KSP Residual norm 1.215826177710e-15 
 15 KSP Residual norm 1.171554989328e-15 
 16 KSP Residual norm 1.131792451315e-15 
 17 KSP Residual norm 1.095821817624e-15 
 18 KSP Residual norm 1.063076346209e-15 
 19 KSP Residual norm 1.033101208235e-15 
 20 KSP Residual norm 1.005526589043e-15 
 21 KSP Residual norm 9.800482913303e-16 
 22 KSP Residual norm 9.564134868581e-16 
 23 KSP Residual norm 9.344100758357e-16 
 24 KSP Residual norm 9.138586218730e-16 
 25 KSP Residual norm 8.946061567343e-16 
 26 KSP Residual norm 8.765213632720e-16 
 27 KSP Residual norm 8.594907882727e-16 
 28 KSP Residual norm 8.434158347032e-16 
 29 KSP Residual norm 8.282103506189e-16 
 30 KSP Residual norm 4.323499537015e-15 
 31 KSP Residual norm 3.089652155242e-16 
 32 KSP Residual norm 4.286687313487e-17 
 33 KSP Residual norm 7.020156691240e-30 
Linear solve converged due to CONVERGED_RTOL iterations 33
KSP Object: 12 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-24, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 12 MPI processes
  type: bjacobi
    number of blocks = 12
    Local solve is same for all blocks, in the following KSP and PC objects:
  KSP Object: (sub_) 1 MPI processes
    type: preonly
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using NONE norm type for convergence test
  PC Object: (sub_) 1 MPI processes
    type: ilu
      out-of-place factorization
      0 levels of fill
      tolerance for zero pivot 2.22045e-14
      matrix ordering: natural
      factor fill ratio given 1., needed 1.
        Factored matrix follows:
          Mat Object: 1 MPI processes
            type: seqbaij
            rows=15352, cols=15352, bs=2
            package used to perform factorization: petsc
            total: nonzeros=272104, allocated nonzeros=272104
            total number of mallocs used during MatSetValues calls=0
                block size is 2
    linear system matrix = precond matrix:
    Mat Object: 1 MPI processes
      type: seqbaij
      rows=15352, cols=15352, bs=2
      total: nonzeros=272104, allocated nonzeros=272104
      total number of mallocs used during MatSetValues calls=0
          block size is 2
  linear system matrix = precond matrix:
  Mat Object: 12 MPI processes
    type: mpibaij
    rows=182408, cols=182408, bs=2
    total: nonzeros=3268864, allocated nonzeros=3268864
    total number of mallocs used during MatSetValues calls=0
  0 KSP Residual norm 0.000000000000e+00 
Linear solve converged due to CONVERGED_ATOL iterations 0
KSP Object: 12 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-24, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 12 MPI processes
  type: bjacobi
    number of blocks = 12
    Local solve is same for all blocks, in the following KSP and PC objects:
  KSP Object: (sub_) 1 MPI processes
    type: preonly
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using NONE norm type for convergence test
  PC Object: (sub_) 1 MPI processes
    type: ilu
      out-of-place factorization
      0 levels of fill
      tolerance for zero pivot 2.22045e-14
      matrix ordering: natural
      factor fill ratio given 1., needed 1.
        Factored matrix follows:
          Mat Object: 1 MPI processes
            type: seqbaij
            rows=15352, cols=15352, bs=2
            package used to perform factorization: petsc
            total: nonzeros=272104, allocated nonzeros=272104
            total number of mallocs used during MatSetValues calls=0
                block size is 2
    linear system matrix = precond matrix:
    Mat Object: 1 MPI processes
      type: seqbaij
      rows=15352, cols=15352, bs=2
      total: nonzeros=272104, allocated nonzeros=272104
      total number of mallocs used during MatSetValues calls=0
          block size is 2
  linear system matrix = precond matrix:
  Mat Object: 12 MPI processes
    type: mpibaij
    rows=182408, cols=182408, bs=2
    total: nonzeros=3268864, allocated nonzeros=3268864
    total number of mallocs used during MatSetValues calls=0

System for L2 projection for exact stress starting 

Current time is 14:09:03 
IGA: dim=2 dof=4 order=2 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[3,2] rule=LEGENDRE[6] periodic=0 nnp=304 nel=301
Axis 1: basis=BSPLINE[3,2] rule=LEGENDRE[6] periodic=0 nnp=304 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=92416 min=7500 max=8034 max/min=1.0712
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347
  0 KSP Residual norm 5.371207815863e+01 
  1 KSP Residual norm 1.857303675812e+01 
  2 KSP Residual norm 6.259943157132e+00 
  3 KSP Residual norm 8.355988694144e+00 
  4 KSP Residual norm 9.799989170485e-02 
  5 KSP Residual norm 1.050505434696e-02 
  6 KSP Residual norm 4.141404141426e-04 
  7 KSP Residual norm 1.070453214982e-04 
  8 KSP Residual norm 1.402174136749e-05 
  9 KSP Residual norm 2.171678641132e-05 
 10 KSP Residual norm 4.394385694141e-06 
 11 KSP Residual norm 1.481137501823e-06 
 12 KSP Residual norm 2.966844596444e-09 
 13 KSP Residual norm 3.908840523189e-10 
 14 KSP Residual norm 1.017171484454e-10 
 15 KSP Residual norm 3.286156670320e-11 
 16 KSP Residual norm 9.425140032209e-15 
 17 KSP Residual norm 2.704016343621e-16 
 18 KSP Residual norm 5.038284708728e-18 
 19 KSP Residual norm 4.750430623636e-19 
 20 KSP Residual norm 9.017687159270e-21 
 21 KSP Residual norm 9.274331686070e-21 
 22 KSP Residual norm 5.722641903828e-21 
 23 KSP Residual norm 1.782657053426e-21 
 24 KSP Residual norm 2.824643962190e-22 
 25 KSP Residual norm 3.749486574598e-24 
 26 KSP Residual norm 5.753232457370e-25 
 27 KSP Residual norm 1.750065018621e-25 
 28 KSP Residual norm 2.720974840958e-28 
 29 KSP Residual norm 2.358449389159e-30 
Linear solve converged due to CONVERGED_RTOL iterations 29
KSP Object: 12 MPI processes
  type: cg
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-30, absolute=1e-45, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 12 MPI processes
  type: bjacobi
    number of blocks = 12
    Local solve is same for all blocks, in the following KSP and PC objects:
  KSP Object: (sub_) 1 MPI processes
    type: preonly
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using NONE norm type for convergence test
  PC Object: (sub_) 1 MPI processes
    type: ilu
      out-of-place factorization
      0 levels of fill
      tolerance for zero pivot 2.22045e-14
      matrix ordering: natural
      factor fill ratio given 1., needed 1.
        Factored matrix follows:
          Mat Object: 1 MPI processes
            type: seqbaij
            rows=30704, cols=30704, bs=4
            package used to perform factorization: petsc
            total: nonzeros=5782400, allocated nonzeros=5782400
            total number of mallocs used during MatSetValues calls=0
                block size is 4
    linear system matrix = precond matrix:
    Mat Object: 1 MPI processes
      type: seqbaij
      rows=30704, cols=30704, bs=4
      total: nonzeros=5782400, allocated nonzeros=5782400
      total number of mallocs used during MatSetValues calls=0
          block size is 4
  linear system matrix = precond matrix:
  Mat Object: 12 MPI processes
    type: mpibaij
    rows=369664, cols=369664, bs=4
    total: nonzeros=71639296, allocated nonzeros=71639296
    total number of mallocs used during MatSetValues calls=0

System for L2 projection for grad(Z0) 

Current time is 14:09:17 
IGA: dim=2 dof=4 order=2 geometry=2 rational=0 property=0
Axis 0: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Axis 1: basis=BSPLINE[3,2] rule=LEGENDRE[4] periodic=0 nnp=304 nel=301
Partition - MPI: processors=[3,4,1] total=12
Partition - nnp: sum=92416 min=7500 max=8034 max/min=1.0712
Partition - nel: sum=90601 min=7500 max=7676 max/min=1.02347
  0 KSP Residual norm 9.848308306720e+00 
  1 KSP Residual norm 3.365614055993e+00 
  2 KSP Residual norm 7.252874457634e-01 
  3 KSP Residual norm 1.209051843879e-01 
  4 KSP Residual norm 1.683734578025e-02 
  5 KSP Residual norm 3.812624139598e-03 
  6 KSP Residual norm 1.128403354165e-04 
  7 KSP Residual norm 2.043508459212e-05 
  8 KSP Residual norm 4.407848115840e-06 
  9 KSP Residual norm 6.571867833065e-06 
 10 KSP Residual norm 1.477667569959e-06 
 11 KSP Residual norm 5.298406831764e-07 
 12 KSP Residual norm 9.127618530091e-10 
 13 KSP Residual norm 1.140169964672e-10 
 14 KSP Residual norm 2.725671076823e-11 
 15 KSP Residual norm 1.149500244208e-11 
 16 KSP Residual norm 3.173328927765e-15 
 17 KSP Residual norm 9.250098187532e-17 
Linear solve converged due to CONVERGED_RTOL iterations 17
KSP Object: 12 MPI processes
  type: cg
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-16, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 12 MPI processes
  type: bjacobi
    number of blocks = 12
    Local solve is same for all blocks, in the following KSP and PC objects:
  KSP Object: (sub_) 1 MPI processes
    type: preonly
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using NONE norm type for convergence test
  PC Object: (sub_) 1 MPI processes
    type: ilu
      out-of-place factorization
      0 levels of fill
      tolerance for zero pivot 2.22045e-14
      matrix ordering: natural
      factor fill ratio given 1., needed 1.
        Factored matrix follows:
          Mat Object: 1 MPI processes
            type: seqbaij
            rows=30704, cols=30704, bs=4
            package used to perform factorization: petsc
            total: nonzeros=5782400, allocated nonzeros=5782400
            total number of mallocs used during MatSetValues calls=0
                block size is 4
    linear system matrix = precond matrix:
    Mat Object: 1 MPI processes
      type: seqbaij
      rows=30704, cols=30704, bs=4
      total: nonzeros=5782400, allocated nonzeros=5782400
      total number of mallocs used during MatSetValues calls=0
          block size is 4
  linear system matrix = precond matrix:
  Mat Object: 12 MPI processes
    type: mpibaij
    rows=369664, cols=369664, bs=4
    total: nonzeros=71639296, allocated nonzeros=71639296
    total number of mallocs used during MatSetValues calls=0
Current time is 14:09:30 
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


./PruebaV5 on a arch-linux2-c-debug named DESKTOP-NCAM2GT with 12 processors, by eazegpi Thu Jul 23 14:09:30 2020
Using Petsc Release Version 3.12.4, unknown 

                         Max       Max/Min     Avg       Total 
Time (sec):           1.235e+03     1.000   1.235e+03
Objects:              1.291e+03     1.000   1.291e+03
Flop:                 3.896e+11     1.119   3.670e+11  4.404e+12
Flop/sec:             3.154e+08     1.119   2.972e+08  3.566e+09
Memory:               1.701e+08     1.310   1.355e+08  1.626e+09
MPI Messages:         3.370e+03     1.844   2.486e+03  2.983e+04
MPI Message Lengths:  9.503e+07     2.479   1.956e+04  5.835e+08
MPI Reductions:       1.091e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.2350e+03 100.0%  4.4041e+12 100.0%  2.983e+04 100.0%  1.956e+04      100.0%  1.090e+04  99.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         88 1.0 2.6908e-01 1.1 0.00e+00 0.0 1.2e+03 4.0e+00 0.0e+00  0  0  4  0  0   0  0  4  0  0     0
BuildTwoSidedF        82 1.0 7.8223e-01 1.4 0.00e+00 0.0 1.6e+03 1.9e+05 0.0e+00  0  0  5 53  0   0  0  5 53  0     0
PCSetUp               18 1.0 2.5736e+02 1.0 2.55e+09 1.1 0.0e+00 0.0e+00 9.0e+01 21  1  0  0  1  21  1  0  0  1   115
PCSetUpOnBlocks        9 1.0 3.4689e+00 1.1 2.39e+09 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  7905
PCApply              254 1.0 5.6867e+00 1.1 3.79e+11 1.1 1.2e+03 3.3e+04 7.5e+01  0 97  4  7  1   0 97  4  7  1 751507
MatMult              241 1.0 3.7649e+00 1.1 1.73e+09 1.1 1.4e+04 3.6e+03 0.0e+00  0  0 47  9  0   0  0 47  9  0  5339
MatSolve             254 1.0 5.6505e+00 1.1 3.79e+11 1.1 1.2e+03 3.3e+04 7.5e+01  0 97  4  7  1   0 97  4  7  1 756303
MatLUFactorSym         4 1.0 1.7793e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.6e+01  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum        11 1.0 2.3898e+02 1.0 2.55e+09 1.1 0.0e+00 0.0e+00 0.0e+00 19  1  0  0  0  19  1  0  0  0   124
MatILUFactorSym        7 1.0 5.1759e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      24 1.0 6.8005e+0115.2 0.00e+00 0.0 5.8e+02 5.4e+05 8.8e+01  4  0  2 53  1   4  0  2 53  1     0
MatAssemblyEnd        24 1.0 1.1524e+00 1.0 1.89e+04 0.0 1.1e+03 9.5e+02 4.0e+02  0  0  4  0  4   0  0  4  0  4     0
MatGetRowIJ            8 1.0 2.7418e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         8 1.0 1.4024e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         1 1.0 2.7559e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView               27 3.0 8.4600e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.8e+01  0  0  0  0  0   0  0  0  0  0     0
IGAFormSystem          1 1.0 1.0324e+01 1.1 2.30e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0  2628
VecView               13 1.0 1.9998e-01 1.0 0.00e+00 0.0 1.4e+02 2.0e+05 1.3e+01  0  0  0  5  0   0  0  0  5  0     0
VecMDot              190 1.0 3.7770e-01 1.2 1.13e+08 1.1 0.0e+00 0.0e+00 3.8e+02  0  0  0  0  3   0  0  0  0  3  3462
VecTDot               92 1.0 2.7244e-01 1.6 5.91e+06 1.1 0.0e+00 0.0e+00 1.8e+02  0  0  0  0  2   0  0  0  0  2   250
VecNorm              501 1.0 4.8219e-01 1.5 2.44e+07 1.1 0.0e+00 0.0e+00 5.1e+02  0  0  0  0  5   0  0  0  0  5   585
VecScale             203 1.0 1.5454e-02 1.2 4.54e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3409
VecCopy               20 1.0 1.6468e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               321 1.0 8.5447e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              113 1.0 2.2312e-02 1.5 6.78e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3502
VecAYPX               44 1.0 1.5193e-02 2.6 2.83e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2141
VecMAXPY             203 1.0 2.3531e-01 1.3 1.21e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5977
VecAssemblyBegin      38 1.0 5.0565e-01 1.6 0.00e+00 0.0 1.0e+03 1.3e+03 7.6e+01  0  0  3  0  1   0  0  3  0  1     0
VecAssemblyEnd        38 1.0 6.0313e-03 8.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecLoad               15 1.0 2.7414e-01 1.0 0.00e+00 0.0 1.6e+02 1.8e+05 6.6e+01  0  0  1  5  1   0  0  1  5  1     0
VecScatterBegin      317 1.0 2.5912e-01 2.2 0.00e+00 0.0 1.6e+04 7.5e+03 7.0e+00  0  0 53 20  0   0  0 53 20  0     0
VecScatterEnd        317 1.0 1.4750e-01 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         206 1.0 2.7243e-01 1.3 1.38e+07 1.1 0.0e+00 0.0e+00 4.1e+02  0  0  0  0  4   0  0  0  0  4   588
SFSetGraph            88 1.0 4.4982e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               92 1.0 4.4453e-01 1.1 0.00e+00 0.0 3.5e+03 4.5e+03 0.0e+00  0  0 12  3  0   0  0 12  3  0     0
SFBcastOpBegin       305 1.0 2.3625e-01 2.4 0.00e+00 0.0 1.6e+04 6.5e+03 7.0e+00  0  0 52 17  0   0  0 52 17  0     0
SFBcastOpEnd         305 1.0 1.3897e-01 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin         12 1.0 2.0040e-02 1.6 0.00e+00 0.0 2.9e+02 6.1e+04 0.0e+00  0  0  1  3  0   0  0  1  3  0     0
SFReduceEnd           12 1.0 5.2962e-03 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp              18 1.0 2.3464e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              13 1.0 2.7091e+02 1.0 3.83e+11 1.1 1.5e+04 6.0e+03 5.5e+03 22 98 51 16 50  22 98 51 16 50 15970
KSPGMRESOrthog       190 1.0 2.5035e+00 1.1 2.25e+08 1.1 0.0e+00 0.0e+00 3.7e+03  0  0  0  0 34   0  0  0  0 34  1045
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

                 IGA    12              1         1224     0.
      Preconditioner    18             18        17384     0.
    Distributed Mesh     7              7        34328     0.
              Matrix    45             45  18446744074214957056     0.
           Index Set   393            382      7328512     0.
   IS L to G Mapping    35             24       791824     0.
              Viewer    39             38        31920     0.
         Vec Scatter    92             48        38016     0.
              Vector   495            455     61809368     0.
   Star Forest Graph   106             62        59840     0.
   Application Order    24             13       805296     0.
       Krylov Solver    18             18       180504     0.
     Discrete System     7              7         6608     0.
========================================================================================================================
Average time to get PetscTime(): 1.0252e-06
Average time for MPI_Barrier(): 0.00080328
Average time for zero size MPI_Send(): 5.07434e-05
#PETSc Option Table entries:
-iga_view
-ksp_converged_reason
-ksp_monitor
-ksp_view
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-cc=gcc --with-fc=gfortran --download-mpich --download-fblaslapack --download-scalapack --download-mumps --download-parmetis --download-metis --download-ptscotch --download-hypre --download-ml --download-cmake
-----------------------------------------
Libraries compiled on 2020-04-30 16:20:45 on DESKTOP-NCAM2GT 
Machine characteristics: Linux-4.4.0-18362-Microsoft-x86_64-with-Ubuntu-20.04-focal
Using PETSc directory: /home/eazegpi/petsc
Using PETSc arch: arch-linux2-c-debug
-----------------------------------------

Using C compiler: /home/eazegpi/petsc/arch-linux2-c-debug/bin/mpicc  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g3  
Using Fortran compiler: /home/eazegpi/petsc/arch-linux2-c-debug/bin/mpif90  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g   
-----------------------------------------

Using include paths: -I/home/eazegpi/petsc/include -I/home/eazegpi/petsc/arch-linux2-c-debug/include
-----------------------------------------

Using C linker: /home/eazegpi/petsc/arch-linux2-c-debug/bin/mpicc
Using Fortran linker: /home/eazegpi/petsc/arch-linux2-c-debug/bin/mpif90
Using libraries: -Wl,-rpath,/home/eazegpi/petsc/arch-linux2-c-debug/lib -L/home/eazegpi/petsc/arch-linux2-c-debug/lib -lpetsc -Wl,-rpath,/home/eazegpi/petsc/arch-linux2-c-debug/lib -L/home/eazegpi/petsc/arch-linux2-c-debug/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/9 -L/usr/lib/gcc/x86_64-linux-gnu/9 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lml -lflapack -lfblas -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -lm -lstdc++ -ldl -lmpifort -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lrt -lstdc++ -ldl
-----------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


